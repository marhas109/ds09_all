{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Solution: Early Stopping and Training Monitoring\n",
    "\n",
    "## Introduction\n",
    "\n",
    "You've recently joined HealthTech Analytics, a healthcare AI startup that's developing systems to predict patient readmission risk based on electronic health records. As a junior data scientist, you've been tasked with implementing a neural network model, which has been showing promising results but has inconsistent performance.\n",
    "\n",
    "Your manager explains that the model sometimes performs well, but other times it overfits to training data or fails to converge properly. She suspects that proper training monitoring and early stopping strategies might solve these issues, making the model more reliable for clinical applications.\n",
    "\n",
    "The Chief Data Officer has emphasized that the company can't afford to waste computational resources on models that aren't learning effectively, and clinical staff need stable, reliable predictions. You'll need to implement proper training monitoring and callbacks to ensure the model trains efficiently and generalizes well to new patient data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Import Libraries and Set Up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, TargetEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Load and Explore Dataset\n",
    "\n",
    "The dataset contains information from diabetic patients with various features such as age, gender, lab results, medical history, and a target variable indicating whether the patient was readmitted within 30 days, after 30 days, or not at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in Data\n",
    "patient_data = None\n",
    "patient_data.info()\n",
    "patient_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore the dataset\n",
    "print(f\"Dataset shape: {patient_data.shape}\")\n",
    "print(f\"Readmission rate: {patient_data['readmitted'].value_counts(normalize=True)}\")\n",
    "\n",
    "# Check class balance\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x='readmitted', data=patient_data)\n",
    "plt.title('Distribution of Readmissions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate out features for visualization\n",
    "num_features = patient_data.select_dtypes(include='number')\n",
    "# Irrelevant or categoricall\n",
    "num_features.drop(['encounter_id', 'patient_nbr', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id'], axis=1, inplace=True)\n",
    "\n",
    "# Look at numerical feature distributions\n",
    "for i, col in enumerate(num_features):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sns.histplot(data=patient_data, x=col, hue='readmitted', kde=True)\n",
    "    plt.title(f'Distribution of {col} by Readmission Status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate out features for visualization\n",
    "categorical = ['admission_type_id', 'discharge_disposition_id', 'admission_source_id']\n",
    "cat_columns = patient_data.select_dtypes(include='object')\n",
    "cat_cols = list(cat_columns.columns)\n",
    "categorical.extend(cat_cols)\n",
    "categorical.remove('readmitted')\n",
    "cat_features = patient_data[categorical]\n",
    "\n",
    "# Categorical features\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(cat_features):\n",
    "    crosstab = pd.crosstab(patient_data[col], patient_data['readmitted'], normalize='index')\n",
    "    crosstab.plot(kind='bar', stacked=True, colormap='viridis')\n",
    "    plt.title(f'{col} vs Readmission Rate')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implement Baseline Model\n",
    "First you need to prepare the data for modeling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "# We will look to combine readmission to make this binary\n",
    "patient_data['readmitted'] = patient_data['readmitted'].map({'<30': 1, '>30': 1, 'NO': 0})\n",
    "\n",
    "# These columns hold no meaning are just unique identifiers and readmitted is our target\n",
    "cols_to_drop = ['encounter_id', 'patient_nbr', 'readmitted']\n",
    "X = None\n",
    "y = None\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split: 80% train+validation, 20% test\n",
    "X_train_val, X_test, y_train_val, y_test = None\n",
    "\n",
    "# Second split: 75% train, 25% validation (resulting in 60% train, 20% validation, 20% test overall)\n",
    "X_train, X_val, y_train, y_val = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "# Preprocess data with Column Transformer pipeline\n",
    "# To prevent high dimenstionality we will target encode the diagnosis codes rather than one hot encode\n",
    "target_encode_cols = ['diag_1', 'diag_2', 'diag_3']\n",
    "ohe_cols = [col for col in categorical if col not in target_encode_cols]\n",
    "num_cols = num_features.columns\n",
    "\n",
    "# Create the preprocessing function\n",
    "num_pipe = Pipeline(steps=[('impute_num', SimpleImputer(strategy='median')),\n",
    "                           ('scaler', StandardScaler())])\n",
    "\n",
    "ohe_pipe = Pipeline(steps=[('impute_cat', SimpleImputer(strategy='constant', fill_value='?')),\n",
    "                           ('ohe', OneHotEncoder(drop='first', handle_unknown='ignore'))])\n",
    "\n",
    "tarenc_pipe = Pipeline(steps=[('impute_cat', SimpleImputer(strategy='constant', fill_value='?')),\n",
    "                              ('tar_encode', TargetEncoder(target_type='binary'))])\n",
    "\n",
    "col_trans = ColumnTransformer(transformers=[('num', num_pipe, num_cols),\n",
    "                                            ('cat', ohe_pipe, ohe_cols),\n",
    "                                            ('tar', tarenc_pipe, target_encode_cols)],\n",
    "                              remainder='passthrough')\n",
    "\n",
    "# Need to provide y_train for the target encoder\n",
    "X_train_pro = col_trans.fit_transform(X_train, y_train)\n",
    "X_val_pro = col_trans.transform(X_val)\n",
    "X_test_pro = col_trans.transform(X_test)\n",
    "\n",
    "print(f\"Training set: {X_train_pro.shape} samples\")\n",
    "print(f\"Validation set: {X_val_pro.shape} samples\")\n",
    "print(f\"Test set: {X_test_pro.shape} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a baseline with two hidden layers, use the relu activation function, select an appropriate number of nodes (64, 32)\n",
    "# Don't forget your output layer for binary classification\n",
    "def create_baseline_model(input_dim):\n",
    "    model = None\n",
    "    \n",
    "    model.compile(\n",
    "        # Use Adam\n",
    "        optimizer=None,\n",
    "        # Select appropriate loss for binary classification\n",
    "        loss=None,\n",
    "        # Evaluate based on recall\n",
    "        metrics=None\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and train the baseline model\n",
    "baseline_model = create_baseline_model(X_train_pro.shape[1])\n",
    "baseline_model.summary()\n",
    "\n",
    "# Train the model without any callbacks\n",
    "baseline_history = baseline_model.fit(\n",
    "    X_train_pro, y_train,\n",
    "    epochs=50,  # Train for a fixed number of epochs\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val_pro, y_val),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Visualize Training and Validation curves\n",
    "Important to visualize our training curves in order to understand model limitations and adapt the next iteration. Particularly important to understand the models bias and variance (over/under fitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the baseline model on testing data\n",
    "baseline_test_loss, baseline_test_recall = None\n",
    "print(f\"Baseline Test Recall: {baseline_test_recall:.4f}\")\n",
    "\n",
    "# Plot the training and validation loss/accuracy curves\n",
    "def plot_training_history(history, title=''):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    None\n",
    "    \n",
    "    # Plot recall\n",
    "    plt.subplot(1, 2, 2)\n",
    "    None\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot baseline model training history\n",
    "plot_training_history(baseline_history, title='Baseline Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Implement Callbacks for Monitoring and Early Stopping\n",
    "\n",
    "Clear sign of model overfitting and gradient problems. Now, let's implement callbacks to monitor training and prevent overfitting. We will also provide a more complex network to attempt to address the overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement EarlyStopping callback\n",
    "early_stopping = None\n",
    "\n",
    "# Implement ModelCheckpoint callback to save the best model\n",
    "checkpoint_filepath = './best_model.keras'\n",
    "model_checkpoint = None\n",
    "\n",
    "# Implement TensorBoard callback for visualization\n",
    "log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = None\n",
    "\n",
    "# Don't change this one\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,        # Reduce learning rate by 80%\n",
    "    patience=3,        # Wait 5 epochs of no improvement\n",
    "    min_lr=0.000001,    # Don't go below this learning rate\n",
    "    verbose=1          # Print when learning rate changes\n",
    ")\n",
    "\n",
    "# Combine all callbacks into a list\n",
    "callbacks = [\n",
    "    early_stopping,\n",
    "    model_checkpoint,\n",
    "    tensorboard_callback,\n",
    "    reduce_lr\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "# Create an improved model with gradient problem mitigation strategies and train with callbacks\n",
    "def create_improved_model(input_dim):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First layer\n",
    "    model.add(Input((input_dim,)))\n",
    "    model.add(Dense(64))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.2))  \n",
    "    \n",
    "    # Second layer\n",
    "    model.add(Dense(32))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Use Adam optimizer with gradient clipping\n",
    "    optimizer = Adam(learning_rate=0.001, clipvalue=.5)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['recall']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and compile the improved model\n",
    "improved_model = create_improved_model(X_train_pro.shape[1])\n",
    "improved_model.summary()\n",
    "\n",
    "# Train the improved model with the callbacks\n",
    "improved_history = improved_model.fit(\n",
    "    X_train_pro, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val_pro, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Analyze Training Results\n",
    "Again, it is always important to look at curves. Here we should see way less overfitting and platued losses which tell us the model has gone about as far as it can go with the data at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training history with early stopping\n",
    "plot_training_history(None)\n",
    "\n",
    "# Load the best model saved by ModelCheckpoint\n",
    "best_model = None\n",
    "\n",
    "# Evaluate the final improved model\n",
    "improved_test_loss, improved_test_recall = None\n",
    "print(f\"Improved Model (Final) Test Recall: {improved_test_recall:.4f}\")\n",
    "\n",
    "# Evaluate the best model (saved by checkpoint)\n",
    "best_test_loss, best_test_recall = best_model.evaluate(X_test_pro, y_test, verbose=1)\n",
    "print(f\"Best Model (Checkpoint) Test Recall: {best_test_recall:.4f}\")\n",
    "\n",
    "# Compare with baseline\n",
    "print(f\"Baseline Test Recall: {baseline_test_recall:.4f}\")\n",
    "print(f\"Early Stopping activated at epoch {len(improved_history.history['loss'])} of 100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorBoard extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Launch TensorBoard\n",
    "%tensorboard --logdir=logs/fit\n",
    "\n",
    "# Note: TensorBoard output will appear in the notebook\n",
    "# We can examine histograms of weights and gradients, model graph,\n",
    "# and other useful visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training complex machine learning models, the relationship between model complexity and data quality is crucial. Even the most sophisticated neural network architecture can plateau if the loss stops decreasing, indicating that the model has reached the limits of what it can learn from the available data. At this point, rather than adding more layers or parameters, the focus should shift to improving data quality, diversity, and relevance to the specific task. Better data—whether that means more accurate labels, more representative samples, or enhanced feature engineering—often proves more valuable than increased model complexity for breaking through performance plateaus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Reflection and Documentation\n",
    "\n",
    "### Question 1: How did early stopping affect the training process and final model performance?\n",
    "\n",
    "Write Your Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: What patterns did you observe in the training and validation curves?\n",
    "\n",
    "Write Your Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: In a healthcare context like this one, why is it particularly important to prevent overfitting?\n",
    "\n",
    "Write Your Answer Here\n",
    "\n",
    "### Question 4: How would you explain the benefits of your monitoring approach to non-technical healthcare staff?\n",
    "\n",
    "Write Your Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Implemented Techniques\n",
    "\n",
    "In this lab, we've implemented and demonstrated several key techniques for improving neural network training:\n",
    "\n",
    "1. **Early Stopping**: Automatically halts training when validation performance stops improving, preventing overfitting and saving computational resources.\n",
    "\n",
    "2. **Model Checkpointing**: Saves the best-performing model during training, ensuring we retain the optimal weights even if training continues past the ideal point.\n",
    "\n",
    "3. **Training Visualization**: Using TensorBoard and custom plotting functions to monitor and interpret the training process in real-time.\n",
    "\n",
    "4. **BatchNormalization**: Stabilizes the distribution of layer inputs during training, helping to prevent vanishing/exploding gradients.\n",
    "\n",
    "5. **Gradient Clipping**: Limits the size of gradient updates to prevent unstable training.\n",
    "\n",
    "6. **Advanced Activation Functions**: Using LeakyReLU instead of standard ReLU to prevent \"dead neurons\" and improve gradient flow.\n",
    "\n",
    "8. **Dropout**: Randomly deactivating neurons during training to prevent overfitting and improve generalization.\n",
    "\n",
    "By combining these techniques, we were able to improve model performance and training stability if only minorly, resulting in a more reliable patient readmission prediction model that would perform better in real-world healthcare settings. Ultimately, because our final model is accounting for potential issues and still not performing as well as we hoped, it becomes a matter of needing better and more data to predict readmission.\n",
    "\n",
    "These monitoring and optimization techniques are applicable across a wide range of deep learning applications, not just healthcare, and should be considered essential components of any robust deep learning workflow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Cohort_Env)",
   "language": "python",
   "name": "cohort_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
