{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Part 1: Text Preprocessing and Exploration"],"metadata":{"id":"V7IWweaN7DDg"}},{"cell_type":"markdown","source":["Let's import the required libraries and load our dataset:"],"metadata":{"id":"sko0fD4j8PXe"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"INc1xMPd6_ag"},"outputs":[],"source":["# Import necessary libraries\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import re\n","import string\n","from collections import Counter\n","\n","# For text preprocessing\n","import nltk\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","from nltk.util import ngrams\n","\n","# Download necessary NLTK data\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('punkt_tab')\n","nltk.download('averaged_perceptron_tagger_eng')\n","\n","# For dataset loading\n","from sklearn.datasets import fetch_20newsgroups\n","\n","# Set random seed for reproducibility\n","np.random.seed(42)\n","\n","# Load a subset of the 20 Newsgroups dataset\n","categories = ['comp.graphics', 'rec.autos', 'sci.space', 'talk.politics.misc']\n","newsgroups = fetch_20newsgroups(subset='train', categories=categories, random_state=42)\n","\n","# Create a DataFrame for easier manipulation\n","df = pd.DataFrame({\n","    'text': newsgroups.data,\n","    'category': [newsgroups.target_names[target] for target in newsgroups.target]\n","})\n","\n","# Preview the data\n","print(f\"Dataset shape: {df.shape}\")\n","print(\"\\nCategory distribution:\")\n","print(df['category'].value_counts())\n","print(\"\\nSample document:\")\n","print(df['text'][10][:500])  # Print first 500 characters of a sample document"]},{"cell_type":"markdown","source":["### Step 1: Basic Data Exploration\n","Let's examine the length characteristics of our documents:"],"metadata":{"id":"_-sI1_LY8KuA"}},{"cell_type":"code","source":["\n"],"metadata":{"id":"aNwzYpF_8U_Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 2: Text Cleaning and Preprocessing Function\n","Let's create a comprehensive text preprocessing function that incorporates all the techniques we've learned:"],"metadata":{"id":"69XtNx_98hco"}},{"cell_type":"code","source":[],"metadata":{"id":"M7H5rdOI8kJZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 3: Apply Preprocessing and Analyze Results\n","Now let's apply our preprocessing function to the dataset and examine the results:"],"metadata":{"id":"IODTKVf-8oqg"}},{"cell_type":"code","source":[],"metadata":{"id":"3_p9JjZM8peG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 4: Token Frequency Analysis\n","Let's analyze the most common words in each category after preprocessing:"],"metadata":{"id":"KG61Yron8-Lj"}},{"cell_type":"code","source":[],"metadata":{"id":"yKO1A7mD8_M2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 5: N-gram Analysis\n","Let's go beyond single words and look at common bigrams and trigrams:"],"metadata":{"id":"x2TXrD659FLk"}},{"cell_type":"code","source":[],"metadata":{"id":"jo6OdiCK9KcO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Part 2: Time Series Analysis and Modeling"],"metadata":{"id":"Zj87c0Bw9PIk"}},{"cell_type":"markdown","source":["### Step 1: Data Loading and Initial Exploration\n","First, let's load the S&P 500 historical data and perform initial exploration."],"metadata":{"id":"dVIegiiq9ws3"}},{"cell_type":"code","source":["# Import necessary libraries\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from statsmodels.tsa.stattools import adfuller\n","from statsmodels.tsa.seasonal import seasonal_decompose\n","from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n","from statsmodels.tsa.arima.model import ARIMA\n","import yfinance as yf\n","import warnings\n","\n","# Suppress warnings for cleaner output\n","warnings.filterwarnings('ignore')\n","\n","# Set plotting style\n","sns.set_theme()\n","\n","# Download S&P 500 data for the last 10 years\n","sp500 = yf.download('^GSPC', start='2013-01-01', end='2022-12-31')\n","\n","# Display the first few rows of the dataset\n","print(\"S&P 500 Dataset:\")\n","print(sp500.head())\n","\n","# Display basic information about the dataset\n","print(\"\\nDataset information:\")\n","print(sp500.info())\n","\n","# Calculate basic summary statistics\n","print(\"\\nSummary Statistics:\")\n","print(sp500['Close'].describe())\n","\n","# Check for missing values\n","print(\"\\nMissing values in each column:\")\n","print(sp500.isnull().sum())\n","\n","# Plot the closing price\n","plt.figure(figsize=(14, 7))\n","plt.plot(sp500.index, sp500['Close'])\n","plt.title('S&P 500 Closing Price (2013-2022)')\n","plt.xlabel('Date')\n","plt.ylabel('Price ($)')\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()\n","\n","# Select the closing price as our primary time series for analysis\n","ts_data = sp500['Close']\n","print(\"\\nSelected time series shape:\", ts_data.shape)"],"metadata":{"id":"llNFNFg69OwX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 2: Time Series Characteristics and Visualization\n","Let's analyze the characteristics of the time series through various visualizations."],"metadata":{"id":"m4T_TCL7-DPB"}},{"cell_type":"code","source":[],"metadata":{"id":"RF5KM7DQ-F6g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 3: Stationarity Testing and Transformation\n","Now, let's test for stationarity using the Dickey-Fuller test and apply transformations to make the data stationary."],"metadata":{"id":"CF8raTqP-Hoy"}},{"cell_type":"code","source":[],"metadata":{"id":"ggXs_3xT-URk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 4: Time Series Decomposition\n","Let's decompose our time series into trend, seasonal, and residual components."],"metadata":{"id":"_WoUfIbO-ecx"}},{"cell_type":"code","source":[],"metadata":{"id":"BfOOyeB8-gkN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 5: Autocorrelation Analysis\n","Now, let's analyze the autocorrelation structure of our stationary series."],"metadata":{"id":"DFP_Ep8r-jrE"}},{"cell_type":"code","source":[],"metadata":{"id":"ume0Z4pe-kzD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 6: Time Series Modeling\n","Finally, let's build and evaluate time series models based on our analysis."],"metadata":{"id":"EhoT_KYX-oor"}},{"cell_type":"code","source":[],"metadata":{"id":"UmCppbtg-pPs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Part 3: Neural Networks Implementation and Optimization"],"metadata":{"id":"_mEB--Gk-t3L"}},{"cell_type":"markdown","source":["### Step 1: Load libraries and prepare the datasets"],"metadata":{"id":"OjG1Y1WP-uwG"}},{"cell_type":"code","source":["# Import necessary libraries\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score, mean_squared_error, confusion_matrix, classification_report\n","from sklearn.datasets import load_digits\n","\n","# For deep learning\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras import layers\n","\n","# For PyTorch\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","# Set random seeds for reproducibility\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","torch.manual_seed(42)\n","\n","# Load Digits dataset for classification\n","digits = load_digits()\n","X_digits = digits.data\n","y_digits = digits.target\n","\n","\n","# Display sample digits\n","plt.figure(figsize=(10, 5))\n","for i in range(10):\n","    plt.subplot(2, 5, i+1)\n","    plt.imshow(digits.images[i], cmap='gray')\n","    plt.title(f'Digit: {digits.target[i]}')\n","    plt.axis('off')\n","plt.tight_layout()\n","plt.show()\n","\n","# Print information about dataset\n","print(f\"Digits dataset: {X_digits.shape[0]} samples, {X_digits.shape[1]} features\")"],"metadata":{"id":"iSGaPSFh-vmw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 2: Preprocess data for neural networks"],"metadata":{"id":"BEo8zEVFAkKX"}},{"cell_type":"code","source":[],"metadata":{"id":"qZolykL6Amvs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 3: Build a basic MLP for digits classification using TensorFlow/Keras"],"metadata":{"id":"6qpU6AYuA9vW"}},{"cell_type":"code","source":[],"metadata":{"id":"qoo0lEvBA_lb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 4: Implement the digits classification model in PyTorch"],"metadata":{"id":"93T-ARKaBGqg"}},{"cell_type":"code","source":[],"metadata":{"id":"f1Pn60fkBKjn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 5: Experiment with different activation functions (TensorFlow)"],"metadata":{"id":"393IbhMjBOg7"}},{"cell_type":"code","source":[],"metadata":{"id":"7Ndxo_oZBPu0"},"execution_count":null,"outputs":[]}]}