{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7IWweaN7DDg"
   },
   "source": [
    "## Part 1: Text Preprocessing and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sko0fD4j8PXe"
   },
   "source": [
    "Let's import the required libraries and load our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "INc1xMPd6_ag"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "# For text preprocessing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For dataset loading\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load a subset of the 20 Newsgroups dataset\n",
    "categories = ['comp.graphics', 'rec.autos', 'sci.space', 'talk.politics.misc']\n",
    "newsgroups = fetch_20newsgroups(subset='train', categories=categories, random_state=42)\n",
    "\n",
    "# Create a DataFrame for easier manipulation\n",
    "df = pd.DataFrame({\n",
    "    'text': newsgroups.data,\n",
    "    'category': [newsgroups.target_names[target] for target in newsgroups.target]\n",
    "})\n",
    "\n",
    "# Preview the data\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nCategory distribution:\")\n",
    "print(df['category'].value_counts())\n",
    "print(\"\\nSample document:\")\n",
    "print(df['text'][10][:500])  # Print first 500 characters of a sample document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-sI1_LY8KuA"
   },
   "source": [
    "### Step 1: Basic Data Exploration\n",
    "Let's examine the length characteristics of our documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNwzYpF_8U_Z"
   },
   "outputs": [],
   "source": [
    "\n",
    "df['post_length'] = df['text'].apply(len)\n",
    "df['word_count'] = df['text'].apply(lambda x: len(str(x).split()))\n",
    "df['sentence_count'] =df['text'].apply(lambda x: len(sent_tokenize(str(x))))\n",
    "\n",
    "# Create visualizations to understand the data\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['word_count'], bins=20)\n",
    "plt.title('Distribution of Newsgroup Post Lengths By Word')\n",
    "plt.xlabel('Word Count')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x=\"category\", y='word_count', data=df)\n",
    "plt.title('Post Length by Category')\n",
    "plt.xlabel('Category')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69XtNx_98hco"
   },
   "source": [
    "### Step 2: Text Cleaning and Preprocessing Function\n",
    "Let's create a comprehensive text preprocessing function that incorporates all the techniques we've learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text_tokens):\n",
    "    # Convert to lowercase\n",
    "    text_tokens = [text_token.lower() for text_token in text_tokens]\n",
    "        \n",
    "    cleaned = []\n",
    "    # Remove punctuation but keep numbers\n",
    "    for text_token in text_tokens:    \n",
    "        no_punc = re.sub(r'[^\\w\\s]', '', text_token)\n",
    "     \n",
    "        if no_punc and not no_punc.isspace():\n",
    "            cleaned.append(no_punc)\n",
    "\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# remove stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token not in stop_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_tokens(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "    \n",
    "    # tag with parts of speech\n",
    "    tokens_tagged = pos_tag(tokens)\n",
    "      \n",
    "    # convert to WordNet POS tags\n",
    "    pos_tokens = [(word[0], get_wordnet_pos(word[1])) for word in tokens_tagged]\n",
    "        \n",
    "    # lemmatize with POS tags\n",
    "    lemmatized = [lemmatizer.lemmatize(word[0], word[1]) for word in pos_tokens]\n",
    "        \n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_clean = df['tokens'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step2_clean = initial_clean.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step3_clean = step2_clean.apply(lemmatize_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step2_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7H5rdOI8kJZ"
   },
   "outputs": [],
   "source": [
    "# Sentence and word tokenization\n",
    "df['sentences'] = df['text'].apply(sent_tokenize)\n",
    "df['tokens'] = df['text'].apply(word_tokenize)\n",
    "\n",
    "def cleaning_preprocessing_pipeline(text):\n",
    "    # initial cleaning of word tokens\n",
    "    def clean_text(text_tokens):\n",
    "        \"\"\"\n",
    "        1. Converts to lowercase\n",
    "        2. Removes HTML tags and special characters\n",
    "        3. Handle common \n",
    "        \"\"\"\n",
    "        # Convert to lowercase\n",
    "        text_tokens = [text_token.lower() for text_token in text_tokens]\n",
    "        \n",
    "        cleaned = []\n",
    "        # Remove punctuation but keep numbers\n",
    "        for text_token in text_tokens:    \n",
    "            no_punc = re.sub(r'[^\\w\\s]', '', text_token)\n",
    "        \n",
    "            if no_punc and not no_punc.isspace():\n",
    "                cleaned.append(no_punc)\n",
    "\n",
    "        return cleaned\n",
    "\n",
    "    # get English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # remove stopwords\n",
    "    def remove_stopwords(tokens):\n",
    "        return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # initialize lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # helper function for lemmatization with POS tagging\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        \"\"\"\n",
    "        Convert NLTK POS tags to WordNet POS tags\n",
    "        \"\"\"\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    # lemmatize\n",
    "    def lemmatize_tokens(tokens):\n",
    "        # tag with parts of speech\n",
    "        tokens_tagged = pos_tag(tokens)\n",
    "        \n",
    "        # convert to WordNet POS tags\n",
    "        pos_tokens = [(word[0], get_wordnet_pos(word[1])) for word in tokens_tagged]\n",
    "        \n",
    "        # lemmatize with POS tags\n",
    "        lemmatized = [lemmatizer.lemmatize(word[0], word[1]) for word in pos_tokens]\n",
    "        \n",
    "        return lemmatized\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IODTKVf-8oqg"
   },
   "source": [
    "### Step 3: Apply Preprocessing and Analyze Results\n",
    "Now let's apply our preprocessing function to the dataset and examine the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_p9JjZM8peG"
   },
   "outputs": [],
   "source": [
    "# apply cleaning pipline to all text\n",
    "df['processed_tokens'] = df['tokens'].apply(cleaning_preprocessing_pipeline)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KG61Yron8-Lj"
   },
   "source": [
    "### Step 4: Token Frequency Analysis\n",
    "Let's analyze the most common words in each category after preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKO1A7mD8_M2"
   },
   "outputs": [],
   "source": [
    "# Assign the count of unique word tokens across all reviews\n",
    "lst_tokens = df['tokens']\n",
    "lst_words = [item for sublist in lst_tokens for item in sublist]\n",
    "\n",
    "dict_all_tokens = {}\n",
    "for word in lst_words:\n",
    "    if word in dict_all_tokens:\n",
    "        dict_all_tokens[word] += 1\n",
    "    else:\n",
    "        dict_all_tokens[word] = 1\n",
    "\n",
    "all_tokens = [(token, count) for token, count in dict_all_tokens.items()]\n",
    "unique_token_count_pre = len(all_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2TXrD659FLk"
   },
   "source": [
    "### Step 5: N-gram Analysis\n",
    "Let's go beyond single words and look at common bigrams and trigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jo6OdiCK9KcO"
   },
   "outputs": [],
   "source": [
    "# Function to generate bigrams and trigrams\n",
    "def generate_ngrams(tokens, n):\n",
    "    \"\"\"\n",
    "    Generate n-grams from a list of tokens\n",
    " \n",
    "    Parameters:\n",
    "    tokens (list): List of tokens\n",
    "    n (int): Size of n-grams to generate\n",
    "        \n",
    "    Returns:\n",
    "    list: List of n-grams as tuples\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    for n in n:\n",
    "        result[f'{n}-grams'] = list(ngrams(tokens, n))\n",
    "    return result\n",
    "\n",
    "# Generate bigrams and trigrams - hint - apply lamdba function\n",
    "b_n = [2]\n",
    "t_n = [3]\n",
    "\n",
    "df['bigrams'] = df['lemmatized_tokens'].apply(lambda x: generate_ngrams(x, b_n))\n",
    "df['trigrams'] = df['lemmatized_tokens'].apply(lambda x: generate_ngrams(x, t_n))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zj87c0Bw9PIk"
   },
   "source": [
    "## Part 2: Time Series Analysis and Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVIegiiq9ws3"
   },
   "source": [
    "### Step 1: Data Loading and Initial Exploration\n",
    "First, let's load the S&P 500 historical data and perform initial exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "llNFNFg69OwX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['^GSPC']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S&P 500 Dataset:\n",
      "Empty DataFrame\n",
      "Columns: [(Adj Close, ^GSPC), (Close, ^GSPC), (High, ^GSPC), (Low, ^GSPC), (Open, ^GSPC), (Volume, ^GSPC)]\n",
      "Index: []\n",
      "\n",
      "Dataset information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 0 entries\n",
      "Data columns (total 6 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   (Adj Close, ^GSPC)  0 non-null      float64\n",
      " 1   (Close, ^GSPC)      0 non-null      float64\n",
      " 2   (High, ^GSPC)       0 non-null      float64\n",
      " 3   (Low, ^GSPC)        0 non-null      float64\n",
      " 4   (Open, ^GSPC)       0 non-null      float64\n",
      " 5   (Volume, ^GSPC)     0 non-null      float64\n",
      "dtypes: float64(6)\n",
      "memory usage: 0.0 bytes\n",
      "None\n",
      "\n",
      "Summary Statistics:\n",
      "Ticker  ^GSPC\n",
      "count     0.0\n",
      "mean      NaN\n",
      "std       NaN\n",
      "min       NaN\n",
      "25%       NaN\n",
      "50%       NaN\n",
      "75%       NaN\n",
      "max       NaN\n",
      "\n",
      "Missing values in each column:\n",
      "Price      Ticker\n",
      "Adj Close  ^GSPC     0.0\n",
      "Close      ^GSPC     0.0\n",
      "High       ^GSPC     0.0\n",
      "Low        ^GSPC     0.0\n",
      "Open       ^GSPC     0.0\n",
      "Volume     ^GSPC     0.0\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected time series shape: (0, 1)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import yfinance as yf\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_theme()\n",
    "\n",
    "# Download S&P 500 data for the last 10 years\n",
    "sp500 = yf.download('^GSPC', start='2013-01-01', end='2022-12-31')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"S&P 500 Dataset:\")\n",
    "print(sp500.head())\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"\\nDataset information:\")\n",
    "print(sp500.info())\n",
    "\n",
    "# Calculate basic summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(sp500['Close'].describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(sp500.isnull().sum())\n",
    "\n",
    "# Plot the closing price\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(sp500.index, sp500['Close'])\n",
    "plt.title('S&P 500 Closing Price (2013-2022)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Select the closing price as our primary time series for analysis\n",
    "ts_data = sp500['Close']\n",
    "print(\"\\nSelected time series shape:\", ts_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4T_TCL7-DPB"
   },
   "source": [
    "### Step 2: Time Series Characteristics and Visualization\n",
    "Let's analyze the characteristics of the time series through various visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RF5KM7DQ-F6g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CF8raTqP-Hoy"
   },
   "source": [
    "### Step 3: Stationarity Testing and Transformation\n",
    "Now, let's test for stationarity using the Dickey-Fuller test and apply transformations to make the data stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggXs_3xT-URk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WoUfIbO-ecx"
   },
   "source": [
    "### Step 4: Time Series Decomposition\n",
    "Let's decompose our time series into trend, seasonal, and residual components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BfOOyeB8-gkN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFP_Ep8r-jrE"
   },
   "source": [
    "### Step 5: Autocorrelation Analysis\n",
    "Now, let's analyze the autocorrelation structure of our stationary series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ume0Z4pe-kzD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhoT_KYX-oor"
   },
   "source": [
    "### Step 6: Time Series Modeling\n",
    "Finally, let's build and evaluate time series models based on our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UmCppbtg-pPs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mEB--Gk-t3L"
   },
   "source": [
    "## Part 3: Neural Networks Implementation and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OjG1Y1WP-uwG"
   },
   "source": [
    "### Step 1: Load libraries and prepare the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iSGaPSFh-vmw"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digits dataset: 1797 samples, 64 features\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, confusion_matrix, classification_report\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# For deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "# For PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load Digits dataset for classification\n",
    "digits = load_digits()\n",
    "X_digits = digits.data / 16\n",
    "y_digits = digits.target\n",
    "\n",
    "\n",
    "# Display sample digits\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(digits.images[i], cmap='gray')\n",
    "    plt.title(f'Digit: {digits.target[i]}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print information about dataset\n",
    "print(f\"Digits dataset: {X_digits.shape[0]} samples, {X_digits.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_digits_df = pd.DataFrame(X_digits)\n",
    "y_digits_df = pd.DataFrame(y_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_digits_nan_count = X_digits_df.isnull().any(axis=1).sum()\n",
    "y_digits_nan_count = y_digits_df.isnull().any(axis=1).sum()\n",
    "X_digits_nan_count, y_digits_nan_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEo8zEVFAkKX"
   },
   "source": [
    "### Step 2: Preprocess data for neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "qZolykL6Amvs"
   },
   "outputs": [],
   "source": [
    "# 1st split: 0.8 train+val 0.2 test\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X_digits, y_digits, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2nd split: 0.75 train 0.25 val\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_digits, y_digits, test_size=0.25, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6qpU6AYuA9vW"
   },
   "source": [
    "### Step 3: Build a basic MLP for digits classification using TensorFlow/Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "qoo0lEvBA_lb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 1.8681 - accuracy: 0.5590 - val_loss: 1.2954 - val_accuracy: 0.7917\n",
      "Epoch 2/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.8958 - accuracy: 0.8523 - val_loss: 0.5562 - val_accuracy: 0.8833\n",
      "Epoch 3/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.4352 - accuracy: 0.9035 - val_loss: 0.3295 - val_accuracy: 0.9222\n",
      "Epoch 4/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.2835 - accuracy: 0.9347 - val_loss: 0.2371 - val_accuracy: 0.9417\n",
      "Epoch 5/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.2139 - accuracy: 0.9532 - val_loss: 0.2028 - val_accuracy: 0.9361\n",
      "Epoch 6/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.1779 - accuracy: 0.9547 - val_loss: 0.1677 - val_accuracy: 0.9472\n",
      "Epoch 7/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.1574 - accuracy: 0.9621 - val_loss: 0.1534 - val_accuracy: 0.9500\n",
      "Epoch 8/30\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.1231 - accuracy: 0.9785 - val_loss: 0.1351 - val_accuracy: 0.9694\n",
      "Epoch 9/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.1179 - accuracy: 0.9718 - val_loss: 0.1258 - val_accuracy: 0.9667\n",
      "Epoch 10/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.1059 - accuracy: 0.9696 - val_loss: 0.1141 - val_accuracy: 0.9667\n",
      "Epoch 11/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0847 - accuracy: 0.9844 - val_loss: 0.1139 - val_accuracy: 0.9667\n",
      "Epoch 12/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0838 - accuracy: 0.9822 - val_loss: 0.1068 - val_accuracy: 0.9806\n",
      "Epoch 13/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0697 - accuracy: 0.9874 - val_loss: 0.0975 - val_accuracy: 0.9694\n",
      "Epoch 14/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0635 - accuracy: 0.9881 - val_loss: 0.1053 - val_accuracy: 0.9722\n",
      "Epoch 15/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0568 - accuracy: 0.9918 - val_loss: 0.0991 - val_accuracy: 0.9667\n",
      "Epoch 16/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0516 - accuracy: 0.9911 - val_loss: 0.0932 - val_accuracy: 0.9750\n",
      "Epoch 17/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0490 - accuracy: 0.9918 - val_loss: 0.0973 - val_accuracy: 0.9694\n",
      "Epoch 18/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0457 - accuracy: 0.9941 - val_loss: 0.0924 - val_accuracy: 0.9750\n",
      "Epoch 19/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0415 - accuracy: 0.9926 - val_loss: 0.0881 - val_accuracy: 0.9778\n",
      "Epoch 20/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0353 - accuracy: 0.9963 - val_loss: 0.0880 - val_accuracy: 0.9778\n",
      "Epoch 21/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0320 - accuracy: 0.9955 - val_loss: 0.0900 - val_accuracy: 0.9778\n",
      "Epoch 22/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0381 - accuracy: 0.9948 - val_loss: 0.0910 - val_accuracy: 0.9694\n",
      "Epoch 23/30\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.0313 - accuracy: 0.9948 - val_loss: 0.0871 - val_accuracy: 0.9750\n",
      "Epoch 24/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.9970 - val_loss: 0.0873 - val_accuracy: 0.9778\n",
      "Epoch 25/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0260 - accuracy: 0.9978 - val_loss: 0.0952 - val_accuracy: 0.9750\n",
      "Epoch 26/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.9948 - val_loss: 0.0881 - val_accuracy: 0.9750\n",
      "Epoch 27/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0228 - accuracy: 0.9963 - val_loss: 0.0878 - val_accuracy: 0.9833\n",
      "Epoch 28/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0181 - accuracy: 0.9978 - val_loss: 0.0859 - val_accuracy: 0.9750\n",
      "Epoch 29/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0159 - accuracy: 0.9978 - val_loss: 0.0881 - val_accuracy: 0.9806\n",
      "Epoch 30/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0160 - accuracy: 0.9985 - val_loss: 0.0846 - val_accuracy: 0.9778\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(128, activation='relu', input_shape=(64,)),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93T-ARKaBGqg"
   },
   "source": [
    "### Step 4: Implement the digits classification model in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "f1Pn60fkBKjn"
   },
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long)  # Long tensor for classification\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_classes=10):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "input_size = X_train_t.shape[1]\n",
    "hidden_size = 64\n",
    "output_size = 10\n",
    "\n",
    "mlp_model = MLPClassifier(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mlp_model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_x, batch_y \u001b[38;5;129;01min\u001b[39;00m train_ds:\n\u001b[0;32m      8\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 9\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmlp_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_y)\n\u001b[0;32m     11\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[34], line 13\u001b[0m, in \u001b[0;36mMLPClassifier.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m     12\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n\u001b[1;32m---> 13\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1672\u001b[0m, in \u001b[0;36mSoftmax.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\torch\\nn\\functional.py:2140\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   2138\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   2139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2140\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2142\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_x, batch_y in train_ds:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp_model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "393IbhMjBOg7"
   },
   "source": [
    "### Step 5: Experiment with different activation functions (TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "7Ndxo_oZBPu0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 1.7614 - accuracy: 0.5672 - val_loss: 1.0969 - val_accuracy: 0.8139\n",
      "Epoch 2/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.7272 - accuracy: 0.8701 - val_loss: 0.4474 - val_accuracy: 0.8917\n",
      "Epoch 3/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.3558 - accuracy: 0.9198 - val_loss: 0.2845 - val_accuracy: 0.9222\n",
      "Epoch 4/30\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.2430 - accuracy: 0.9399 - val_loss: 0.2167 - val_accuracy: 0.9444\n",
      "Epoch 5/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.1876 - accuracy: 0.9562 - val_loss: 0.1896 - val_accuracy: 0.9417\n",
      "Epoch 6/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.1615 - accuracy: 0.9577 - val_loss: 0.1603 - val_accuracy: 0.9417\n",
      "Epoch 7/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.1427 - accuracy: 0.9666 - val_loss: 0.1476 - val_accuracy: 0.9556\n",
      "Epoch 8/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.1142 - accuracy: 0.9748 - val_loss: 0.1428 - val_accuracy: 0.9639\n",
      "Epoch 9/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.1096 - accuracy: 0.9718 - val_loss: 0.1292 - val_accuracy: 0.9639\n",
      "Epoch 10/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0957 - accuracy: 0.9733 - val_loss: 0.1275 - val_accuracy: 0.9667\n",
      "Epoch 11/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0818 - accuracy: 0.9822 - val_loss: 0.1240 - val_accuracy: 0.9667\n",
      "Epoch 12/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0864 - accuracy: 0.9785 - val_loss: 0.1204 - val_accuracy: 0.9722\n",
      "Epoch 13/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0675 - accuracy: 0.9859 - val_loss: 0.1099 - val_accuracy: 0.9667\n",
      "Epoch 14/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0604 - accuracy: 0.9866 - val_loss: 0.1200 - val_accuracy: 0.9639\n",
      "Epoch 15/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0563 - accuracy: 0.9866 - val_loss: 0.1158 - val_accuracy: 0.9667\n",
      "Epoch 16/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0542 - accuracy: 0.9881 - val_loss: 0.1095 - val_accuracy: 0.9667\n",
      "Epoch 17/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0504 - accuracy: 0.9881 - val_loss: 0.1145 - val_accuracy: 0.9667\n",
      "Epoch 18/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0473 - accuracy: 0.9918 - val_loss: 0.1189 - val_accuracy: 0.9639\n",
      "Epoch 19/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0448 - accuracy: 0.9903 - val_loss: 0.1040 - val_accuracy: 0.9778\n",
      "Epoch 20/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0380 - accuracy: 0.9926 - val_loss: 0.1054 - val_accuracy: 0.9778\n",
      "Epoch 21/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0351 - accuracy: 0.9941 - val_loss: 0.1143 - val_accuracy: 0.9694\n",
      "Epoch 22/30\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.0378 - accuracy: 0.9933 - val_loss: 0.1077 - val_accuracy: 0.9667\n",
      "Epoch 23/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0364 - accuracy: 0.9911 - val_loss: 0.1092 - val_accuracy: 0.9750\n",
      "Epoch 24/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0290 - accuracy: 0.9963 - val_loss: 0.1069 - val_accuracy: 0.9778\n",
      "Epoch 25/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0275 - accuracy: 0.9948 - val_loss: 0.1116 - val_accuracy: 0.9667\n",
      "Epoch 26/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0303 - accuracy: 0.9926 - val_loss: 0.1060 - val_accuracy: 0.9778\n",
      "Epoch 27/30\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.0240 - accuracy: 0.9963 - val_loss: 0.1041 - val_accuracy: 0.9722\n",
      "Epoch 28/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0215 - accuracy: 0.9978 - val_loss: 0.1026 - val_accuracy: 0.9750\n",
      "Epoch 29/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0183 - accuracy: 0.9970 - val_loss: 0.1075 - val_accuracy: 0.9750\n",
      "Epoch 30/30\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.0178 - accuracy: 0.9978 - val_loss: 0.1098 - val_accuracy: 0.9722\n"
     ]
    }
   ],
   "source": [
    "model2 = keras.Sequential([\n",
    "    keras.layers.Dense(128, activation='LeakyReLU', input_shape=(64,)),\n",
    "    keras.layers.Dense(64, activation='LeakyReLU'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history2 = model2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 1.6891 - accuracy: 0.5761 - val_loss: 0.9995 - val_accuracy: 0.8361\n",
      "Epoch 2/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.7188 - accuracy: 0.8909 - val_loss: 0.4756 - val_accuracy: 0.9056\n",
      "Epoch 3/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.4123 - accuracy: 0.9302 - val_loss: 0.3102 - val_accuracy: 0.9333\n",
      "Epoch 4/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.2891 - accuracy: 0.9443 - val_loss: 0.2372 - val_accuracy: 0.9417\n",
      "Epoch 5/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.2244 - accuracy: 0.9599 - val_loss: 0.1955 - val_accuracy: 0.9528\n",
      "Epoch 6/30\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 0.1866 - accuracy: 0.9599 - val_loss: 0.1601 - val_accuracy: 0.9556\n",
      "Epoch 7/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.1682 - accuracy: 0.9651 - val_loss: 0.1488 - val_accuracy: 0.9556\n",
      "Epoch 8/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.1366 - accuracy: 0.9748 - val_loss: 0.1280 - val_accuracy: 0.9806\n",
      "Epoch 9/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.1245 - accuracy: 0.9748 - val_loss: 0.1178 - val_accuracy: 0.9722\n",
      "Epoch 10/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.1106 - accuracy: 0.9814 - val_loss: 0.1130 - val_accuracy: 0.9833\n",
      "Epoch 11/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0963 - accuracy: 0.9822 - val_loss: 0.1112 - val_accuracy: 0.9694\n",
      "Epoch 12/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0918 - accuracy: 0.9829 - val_loss: 0.1074 - val_accuracy: 0.9722\n",
      "Epoch 13/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0813 - accuracy: 0.9852 - val_loss: 0.1015 - val_accuracy: 0.9722\n",
      "Epoch 14/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0743 - accuracy: 0.9859 - val_loss: 0.1046 - val_accuracy: 0.9722\n",
      "Epoch 15/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0681 - accuracy: 0.9889 - val_loss: 0.1021 - val_accuracy: 0.9722\n",
      "Epoch 16/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0622 - accuracy: 0.9889 - val_loss: 0.0969 - val_accuracy: 0.9750\n",
      "Epoch 17/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0571 - accuracy: 0.9896 - val_loss: 0.0954 - val_accuracy: 0.9750\n",
      "Epoch 18/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0518 - accuracy: 0.9926 - val_loss: 0.0955 - val_accuracy: 0.9750\n",
      "Epoch 19/30\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9903 - val_loss: 0.0875 - val_accuracy: 0.9778\n",
      "Epoch 20/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0431 - accuracy: 0.9948 - val_loss: 0.0900 - val_accuracy: 0.9778\n",
      "Epoch 21/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0405 - accuracy: 0.9955 - val_loss: 0.0929 - val_accuracy: 0.9750\n",
      "Epoch 22/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0421 - accuracy: 0.9933 - val_loss: 0.0943 - val_accuracy: 0.9694\n",
      "Epoch 23/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0370 - accuracy: 0.9941 - val_loss: 0.0861 - val_accuracy: 0.9750\n",
      "Epoch 24/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0324 - accuracy: 0.9970 - val_loss: 0.0874 - val_accuracy: 0.9778\n",
      "Epoch 25/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0317 - accuracy: 0.9963 - val_loss: 0.0902 - val_accuracy: 0.9750\n",
      "Epoch 26/30\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.0444 - accuracy: 0.9926 - val_loss: 0.0925 - val_accuracy: 0.9750\n",
      "Epoch 27/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0278 - accuracy: 0.9955 - val_loss: 0.0856 - val_accuracy: 0.9722\n",
      "Epoch 28/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0251 - accuracy: 0.9970 - val_loss: 0.0838 - val_accuracy: 0.9750\n",
      "Epoch 29/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0220 - accuracy: 0.9978 - val_loss: 0.0890 - val_accuracy: 0.9722\n",
      "Epoch 30/30\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.0215 - accuracy: 0.9978 - val_loss: 0.0883 - val_accuracy: 0.9750\n"
     ]
    }
   ],
   "source": [
    "model3 = keras.Sequential([\n",
    "    keras.layers.Dense(128, activation='tanh', input_shape=(64,)),\n",
    "    keras.layers.Dense(64, activation='tanh'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model3.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history3 = model3.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ai-environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
