{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7IWweaN7DDg"
   },
   "source": [
    "## Part 1: Text Preprocessing and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sko0fD4j8PXe"
   },
   "source": [
    "Let's import the required libraries and load our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "INc1xMPd6_ag"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "# For text preprocessing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For dataset loading\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load a subset of the 20 Newsgroups dataset\n",
    "categories = ['comp.graphics', 'rec.autos', 'sci.space', 'talk.politics.misc']\n",
    "newsgroups = fetch_20newsgroups(subset='train', categories=categories, random_state=42)\n",
    "\n",
    "# Create a DataFrame for easier manipulation\n",
    "df = pd.DataFrame({\n",
    "    'text': newsgroups.data,\n",
    "    'category': [newsgroups.target_names[target] for target in newsgroups.target]\n",
    "})\n",
    "\n",
    "# Preview the data\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nCategory distribution:\")\n",
    "print(df['category'].value_counts())\n",
    "print(\"\\nSample document:\")\n",
    "print(df['text'][10][:500])  # Print first 500 characters of a sample document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-sI1_LY8KuA"
   },
   "source": [
    "### Step 1: Basic Data Exploration\n",
    "Let's examine the length characteristics of our documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNwzYpF_8U_Z"
   },
   "outputs": [],
   "source": [
    "\n",
    "df['post_length'] = df['text'].apply(len)\n",
    "df['word_count'] = df['text'].apply(lambda x: len(str(x).split()))\n",
    "df['sentence_count'] =df['text'].apply(lambda x: len(sent_tokenize(str(x))))\n",
    "\n",
    "# Create visualizations to understand the data\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['word_count'], bins=20)\n",
    "plt.title('Distribution of Newsgroup Post Lengths By Word')\n",
    "plt.xlabel('Word Count')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x=\"category\", y='word_count', data=df)\n",
    "plt.title('Post Length by Category')\n",
    "plt.xlabel('Category')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69XtNx_98hco"
   },
   "source": [
    "### Step 2: Text Cleaning and Preprocessing Function\n",
    "Let's create a comprehensive text preprocessing function that incorporates all the techniques we've learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text_tokens):\n",
    "    # Convert to lowercase\n",
    "    text_tokens = [text_token.lower() for text_token in text_tokens]\n",
    "        \n",
    "    cleaned = []\n",
    "    # Remove punctuation but keep numbers\n",
    "    for text_token in text_tokens:    \n",
    "        no_punc = re.sub(r'[^\\w\\s]', '', text_token)\n",
    "     \n",
    "        if no_punc and not no_punc.isspace():\n",
    "            cleaned.append(no_punc)\n",
    "\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# remove stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token not in stop_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_tokens(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "    \n",
    "    # tag with parts of speech\n",
    "    tokens_tagged = pos_tag(tokens)\n",
    "      \n",
    "    # convert to WordNet POS tags\n",
    "    pos_tokens = [(word[0], get_wordnet_pos(word[1])) for word in tokens_tagged]\n",
    "        \n",
    "    # lemmatize with POS tags\n",
    "    lemmatized = [lemmatizer.lemmatize(word[0], word[1]) for word in pos_tokens]\n",
    "        \n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_clean = df['tokens'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step2_clean = initial_clean.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step3_clean = step2_clean.apply(lemmatize_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step2_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7H5rdOI8kJZ"
   },
   "outputs": [],
   "source": [
    "# Sentence and word tokenization\n",
    "df['sentences'] = df['text'].apply(sent_tokenize)\n",
    "df['tokens'] = df['text'].apply(word_tokenize)\n",
    "\n",
    "def cleaning_preprocessing_pipeline(text):\n",
    "    # initial cleaning of word tokens\n",
    "    def clean_text(text_tokens):\n",
    "        \"\"\"\n",
    "        1. Converts to lowercase\n",
    "        2. Removes HTML tags and special characters\n",
    "        3. Handle common \n",
    "        \"\"\"\n",
    "        # Convert to lowercase\n",
    "        text_tokens = [text_token.lower() for text_token in text_tokens]\n",
    "        \n",
    "        cleaned = []\n",
    "        # Remove punctuation but keep numbers\n",
    "        for text_token in text_tokens:    \n",
    "            no_punc = re.sub(r'[^\\w\\s]', '', text_token)\n",
    "        \n",
    "            if no_punc and not no_punc.isspace():\n",
    "                cleaned.append(no_punc)\n",
    "\n",
    "        return cleaned\n",
    "\n",
    "    # get English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # remove stopwords\n",
    "    def remove_stopwords(tokens):\n",
    "        return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # initialize lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # helper function for lemmatization with POS tagging\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        \"\"\"\n",
    "        Convert NLTK POS tags to WordNet POS tags\n",
    "        \"\"\"\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    # lemmatize\n",
    "    def lemmatize_tokens(tokens):\n",
    "        # tag with parts of speech\n",
    "        tokens_tagged = pos_tag(tokens)\n",
    "        \n",
    "        # convert to WordNet POS tags\n",
    "        pos_tokens = [(word[0], get_wordnet_pos(word[1])) for word in tokens_tagged]\n",
    "        \n",
    "        # lemmatize with POS tags\n",
    "        lemmatized = [lemmatizer.lemmatize(word[0], word[1]) for word in pos_tokens]\n",
    "        \n",
    "        return lemmatized\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IODTKVf-8oqg"
   },
   "source": [
    "### Step 3: Apply Preprocessing and Analyze Results\n",
    "Now let's apply our preprocessing function to the dataset and examine the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_p9JjZM8peG"
   },
   "outputs": [],
   "source": [
    "# apply cleaning pipline to all text\n",
    "df['processed_tokens'] = df['tokens'].apply(cleaning_preprocessing_pipeline)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KG61Yron8-Lj"
   },
   "source": [
    "### Step 4: Token Frequency Analysis\n",
    "Let's analyze the most common words in each category after preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKO1A7mD8_M2"
   },
   "outputs": [],
   "source": [
    "# Assign the count of unique word tokens across all reviews\n",
    "lst_tokens = df['tokens']\n",
    "lst_words = [item for sublist in lst_tokens for item in sublist]\n",
    "\n",
    "dict_all_tokens = {}\n",
    "for word in lst_words:\n",
    "    if word in dict_all_tokens:\n",
    "        dict_all_tokens[word] += 1\n",
    "    else:\n",
    "        dict_all_tokens[word] = 1\n",
    "\n",
    "all_tokens = [(token, count) for token, count in dict_all_tokens.items()]\n",
    "unique_token_count_pre = len(all_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2TXrD659FLk"
   },
   "source": [
    "### Step 5: N-gram Analysis\n",
    "Let's go beyond single words and look at common bigrams and trigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jo6OdiCK9KcO"
   },
   "outputs": [],
   "source": [
    "# Function to generate bigrams and trigrams\n",
    "def generate_ngrams(tokens, n):\n",
    "    \"\"\"\n",
    "    Generate n-grams from a list of tokens\n",
    " \n",
    "    Parameters:\n",
    "    tokens (list): List of tokens\n",
    "    n (int): Size of n-grams to generate\n",
    "        \n",
    "    Returns:\n",
    "    list: List of n-grams as tuples\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    for n in n:\n",
    "        result[f'{n}-grams'] = list(ngrams(tokens, n))\n",
    "    return result\n",
    "\n",
    "# Generate bigrams and trigrams - hint - apply lamdba function\n",
    "b_n = [2]\n",
    "t_n = [3]\n",
    "\n",
    "df['bigrams'] = df['lemmatized_tokens'].apply(lambda x: generate_ngrams(x, b_n))\n",
    "df['trigrams'] = df['lemmatized_tokens'].apply(lambda x: generate_ngrams(x, t_n))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zj87c0Bw9PIk"
   },
   "source": [
    "## Part 2: Time Series Analysis and Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVIegiiq9ws3"
   },
   "source": [
    "### Step 1: Data Loading and Initial Exploration\n",
    "First, let's load the S&P 500 historical data and perform initial exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "llNFNFg69OwX"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import yfinance as yf\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_theme()\n",
    "\n",
    "# Download S&P 500 data for the last 10 years\n",
    "sp500 = yf.download('^GSPC', start='2013-01-01', end='2022-12-31')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"S&P 500 Dataset:\")\n",
    "print(sp500.head())\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"\\nDataset information:\")\n",
    "print(sp500.info())\n",
    "\n",
    "# Calculate basic summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(sp500['Close'].describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(sp500.isnull().sum())\n",
    "\n",
    "# Plot the closing price\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(sp500.index, sp500['Close'])\n",
    "plt.title('S&P 500 Closing Price (2013-2022)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Select the closing price as our primary time series for analysis\n",
    "ts_data = sp500['Close']\n",
    "print(\"\\nSelected time series shape:\", ts_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4T_TCL7-DPB"
   },
   "source": [
    "### Step 2: Time Series Characteristics and Visualization\n",
    "Let's analyze the characteristics of the time series through various visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RF5KM7DQ-F6g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CF8raTqP-Hoy"
   },
   "source": [
    "### Step 3: Stationarity Testing and Transformation\n",
    "Now, let's test for stationarity using the Dickey-Fuller test and apply transformations to make the data stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggXs_3xT-URk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WoUfIbO-ecx"
   },
   "source": [
    "### Step 4: Time Series Decomposition\n",
    "Let's decompose our time series into trend, seasonal, and residual components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BfOOyeB8-gkN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFP_Ep8r-jrE"
   },
   "source": [
    "### Step 5: Autocorrelation Analysis\n",
    "Now, let's analyze the autocorrelation structure of our stationary series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ume0Z4pe-kzD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhoT_KYX-oor"
   },
   "source": [
    "### Step 6: Time Series Modeling\n",
    "Finally, let's build and evaluate time series models based on our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UmCppbtg-pPs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mEB--Gk-t3L"
   },
   "source": [
    "## Part 3: Neural Networks Implementation and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OjG1Y1WP-uwG"
   },
   "source": [
    "### Step 1: Load libraries and prepare the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iSGaPSFh-vmw"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, confusion_matrix, classification_report\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# For deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "# For PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load Digits dataset for classification\n",
    "digits = load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "\n",
    "# Display sample digits\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(digits.images[i], cmap='gray')\n",
    "    plt.title(f'Digit: {digits.target[i]}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print information about dataset\n",
    "print(f\"Digits dataset: {X_digits.shape[0]} samples, {X_digits.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEo8zEVFAkKX"
   },
   "source": [
    "### Step 2: Preprocess data for neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZolykL6Amvs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6qpU6AYuA9vW"
   },
   "source": [
    "### Step 3: Build a basic MLP for digits classification using TensorFlow/Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qoo0lEvBA_lb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93T-ARKaBGqg"
   },
   "source": [
    "### Step 4: Implement the digits classification model in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1Pn60fkBKjn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "393IbhMjBOg7"
   },
   "source": [
    "### Step 5: Experiment with different activation functions (TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Ndxo_oZBPu0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ai-environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
