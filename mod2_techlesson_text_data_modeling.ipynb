{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbbb6b31",
   "metadata": {},
   "source": [
    "In this technical lesson, we'll walk through the process of building a text classification model using Naive Bayes, extending our approach to include word embeddings for comparison with traditional vectorization techniques. We'll follow the established process of:\n",
    "\n",
    "1. problem definition\n",
    "2. preprocessing\n",
    "3. feature extraction\n",
    "4. model selection\n",
    "5. training, and evaluation\n",
    "\n",
    "Customer support departments often receive hundreds or thousands of tickets daily that need to be routed to the correct team. Manually sorting these tickets is time-consuming and prone to error. By automating this classification process, we can significantly reduce response times and ensure that customer issues are handled by the right specialists.\n",
    "\n",
    "We'll implement a Multinomial Naive Bayes classifier, which is particularly well-suited for text classification tasks due to its ability to work with the discrete features created by text vectorization. \n",
    "\n",
    "We'll also explore how modern word embeddings can provide an alternative feature representation that captures semantic relationships between words. \n",
    "\n",
    "Throughout this lesson, you'll learn how to transform raw text into numerical features using different approaches and build probabilistic models that can automatically categorize new text based on patterns learned from labeled examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2c2c02",
   "metadata": {},
   "source": [
    "##### Step 1: Problem Definition and Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2288842b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5ab26d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticket_id</th>\n",
       "      <th>ticket_text</th>\n",
       "      <th>department</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Cannot login to my account after password reset</td>\n",
       "      <td>technical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The website is loading very slowly on my browser</td>\n",
       "      <td>technical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>The app crashes whenever I try to upload photos</td>\n",
       "      <td>technical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Videos are not playing properly on my device</td>\n",
       "      <td>technical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>The checkout process gave me an error</td>\n",
       "      <td>technical</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ticket_id                                       ticket_text department\n",
       "0          1   Cannot login to my account after password reset  technical\n",
       "1          2  The website is loading very slowly on my browser  technical\n",
       "2          3   The app crashes whenever I try to upload photos  technical\n",
       "3          4      Videos are not playing properly on my device  technical\n",
       "4          5             The checkout process gave me an error  technical"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('customer_support_ticket.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9f77f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (90, 3)\n",
      "\n",
      "Class distribution:\n",
      "department\n",
      "technical    30\n",
      "account      30\n",
      "billing      30\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample tickets per department:\n",
      "\n",
      "TECHNICAL: Cannot login to my account after password reset\n",
      "\n",
      "ACCOUNT: How do I update my shipping address for my order?\n",
      "\n",
      "BILLING: My payment was charged twice for one order\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df['department'].value_counts())\n",
    "print(\"\\nSample tickets per department:\")\n",
    "for dept in df['department'].unique():\n",
    "    sample_idx = df[df['department'] == dept].index[0]\n",
    "    print(f\"\\n{dept.upper()}: {df.loc[sample_idx, 'ticket_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838449c9",
   "metadata": {},
   "source": [
    "##### Step 2: Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2afca90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\marha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\marha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\marha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\marha/nltk_data'\n    - 'c:\\\\Users\\\\marha\\\\.conda\\\\envs\\\\ai-environment\\\\nltk_data'\n    - 'c:\\\\Users\\\\marha\\\\.conda\\\\envs\\\\ai-environment\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\marha\\\\.conda\\\\envs\\\\ai-environment\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\marha\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 62\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Apply preprocessing to the dataset\u001b[39;00m\n\u001b[0;32m     61\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mticket_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(basic_preprocess)\n\u001b[1;32m---> 62\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemmatized_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mticket_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43madvanced_preprocess\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Show the preprocessing results for a sample ticket\u001b[39;00m\n\u001b[0;32m     66\u001b[0m sample_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\pandas\\core\\series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4527\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4529\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\pandas\\core\\apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\pandas\\core\\apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[4], line 50\u001b[0m, in \u001b[0;36madvanced_preprocess\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     47\u001b[0m tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(text)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Tag with pos\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m tokens_tagged \u001b[38;5;241m=\u001b[39m \u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m pos_tokens \u001b[38;5;241m=\u001b[39m [(word[\u001b[38;5;241m0\u001b[39m], get_wordnet_pos(word[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens_tagged]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Remove stopwords and lemmatize\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\nltk\\tag\\__init__.py:168\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpos_tag\u001b[39m(tokens, tagset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    144\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;124;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;124;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[1;32mc:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\nltk\\tag\\__init__.py:110\u001b[0m, in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    108\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m PerceptronTagger(lang\u001b[38;5;241m=\u001b[39mlang)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "File \u001b[1;32mc:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\nltk\\tag\\perceptron.py:180\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[1;34m(self, load, lang, loc)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m=\u001b[39m path_join(\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTRAINED_TAGGER_PATH, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTAGGER_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    178\u001b[0m )\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[1;32m--> 180\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\nltk\\tag\\perceptron.py:277\u001b[0m, in \u001b[0;36mPerceptronTagger.load_from_json\u001b[1;34m(self, lang, loc)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_from_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m, loc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;66;03m# Automatically find path to the tagger if location is not specified.\u001b[39;00m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m loc:\n\u001b[1;32m--> 277\u001b[0m         loc \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtaggers/averaged_perceptron_tagger_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_param\u001b[39m(json_file):\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path_join(loc, json_file)) \u001b[38;5;28;01mas\u001b[39;00m fin:\n",
      "File \u001b[1;32mc:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\marha/nltk_data'\n    - 'c:\\\\Users\\\\marha\\\\.conda\\\\envs\\\\ai-environment\\\\nltk_data'\n    - 'c:\\\\Users\\\\marha\\\\.conda\\\\envs\\\\ai-environment\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\marha\\\\.conda\\\\envs\\\\ai-environment\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\marha\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# Lemmatizer relies of part of speech to help\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "  '''\n",
    "  Translate nltk POS to wordnet tags\n",
    "  '''\n",
    "  if treebank_tag.startswith('J'):\n",
    "      return wordnet.ADJ\n",
    "  elif treebank_tag.startswith('V'):\n",
    "      return wordnet.VERB\n",
    "  elif treebank_tag.startswith('N'):\n",
    "      return wordnet.NOUN\n",
    "  elif treebank_tag.startswith('R'):\n",
    "      return wordnet.ADV\n",
    "  else:\n",
    "      return wordnet.NOUN\n",
    "\n",
    "\n",
    "def basic_preprocess(text):\n",
    "   \"\"\"Basic preprocessing function for text.\"\"\"\n",
    "   # Convert to lowercase\n",
    "   text = text.lower()\n",
    "  \n",
    "   # Remove special characters and numbers\n",
    "   text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "  \n",
    "   # Return cleaned text\n",
    "   return text\n",
    "\n",
    "\n",
    "def advanced_preprocess(text):\n",
    "   \"\"\"Advanced preprocessing with tokenization, stopword removal, and lemmatization.\"\"\"\n",
    "   # Basic cleaning\n",
    "   text = basic_preprocess(text)\n",
    "  \n",
    "   # Tokenize\n",
    "   tokens = nltk.word_tokenize(text)\n",
    "  \n",
    "   # Tag with pos\n",
    "   tokens_tagged = pos_tag(tokens)\n",
    "   pos_tokens = [(word[0], get_wordnet_pos(word[1])) for word in tokens_tagged]\n",
    "  \n",
    "   # Remove stopwords and lemmatize\n",
    "   cleaned_tokens = [lemmatizer.lemmatize(token[0], token[1]) for token in pos_tokens if token[0] not in stop_words and len(token[0]) > 1]\n",
    "  \n",
    "   # Return cleaned tokens\n",
    "   return ' '.join(cleaned_tokens)\n",
    "\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "df['cleaned_text'] = df['ticket_text'].apply(basic_preprocess)\n",
    "df['lemmatized_text'] = df['ticket_text'].apply(advanced_preprocess)\n",
    "\n",
    "\n",
    "# Show the preprocessing results for a sample ticket\n",
    "sample_idx = 1\n",
    "print(f\"Original: {df.loc[sample_idx, 'ticket_text']}\")\n",
    "print(f\"Cleaned: {df.loc[sample_idx, 'cleaned_text']}\")\n",
    "print(f\"Lemmatized: {df.loc[sample_idx, 'lemmatized_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8662c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 63\n",
      "Testing set size: 27\n",
      "Class distribution in training set: \n",
      "department\n",
      "technical    21\n",
      "account      21\n",
      "billing      21\n",
      "Name: count, dtype: int64\n",
      "Class distribution in testing set: \n",
      "department\n",
      "account      9\n",
      "technical    9\n",
      "billing      9\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "   df['ticket_text'],\n",
    "   df['department'],\n",
    "   test_size=0.3,\n",
    "   random_state=42,\n",
    "   stratify=df['department']  # Ensure balanced classes in both sets\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Testing set size: {len(X_test)}\")\n",
    "print(f\"Class distribution in training set: \\n{y_train.value_counts()}\")\n",
    "print(f\"Class distribution in testing set: \\n{y_test.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b080deaa",
   "metadata": {},
   "source": [
    "##### Step 3: Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ca2727f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words features: 57\n",
      "TF-IDF features: 57\n",
      "Sample BoW features: ['access' 'account' 'account password' 'account show' 'add' 'address'\n",
      " 'app' 'billing' 'browser' 'change']\n",
      "Sample TF-IDF features (including bigrams): ['account password', 'account show', 'get error']\n"
     ]
    }
   ],
   "source": [
    "# Bag of Words vectorizer\n",
    "count_vectorizer = CountVectorizer(\n",
    "   preprocessor=advanced_preprocess,\n",
    "   lowercase=False,  # Already done in preprocessing\n",
    "   min_df=2,  # Ignore terms that appear in fewer than 2 documents\n",
    "   max_df=0.95, # Ignore terms that appear in more than 95% of documents\n",
    "   ngram_range=(1, 2) # Include both single words and pairs of consecutive words\n",
    ")\n",
    "\n",
    "# TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "   preprocessor=advanced_preprocess,\n",
    "   lowercase=False,\n",
    "   min_df=2,\n",
    "   max_df=0.95,\n",
    "   ngram_range=(1, 2)\n",
    ")\n",
    "\n",
    "# Apply vectorizers to training data\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Get feature information\n",
    "count_features = count_vectorizer.get_feature_names_out()\n",
    "tfidf_features = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"Bag of Words features: {len(count_features)}\")\n",
    "print(f\"TF-IDF features: {len(tfidf_features)}\")\n",
    "print(f\"Sample BoW features: {count_features[:10]}\")\n",
    "print(f\"Sample TF-IDF features (including bigrams): {[f for f in tfidf_features[:20] if ' ' in f][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be29fe7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 58\u001b[0m\n\u001b[0;32m     54\u001b[0m    \u001b[38;5;28;01mreturn\u001b[39;00m doc_vector\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Create document vectors for training and test sets\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m X_train_w2v \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([document_to_vector(tokens, w2v_model) \u001b[38;5;28;01mfor\u001b[39;00m tokens \u001b[38;5;129;01min\u001b[39;00m \u001b[43mX_train_tokens\u001b[49m])\n\u001b[0;32m     59\u001b[0m X_test_w2v \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([document_to_vector(tokens, w2v_model) \u001b[38;5;28;01mfor\u001b[39;00m tokens \u001b[38;5;129;01min\u001b[39;00m X_test_tokens])\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord2Vec document vectors shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train_w2v\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "# Train a Word2Vec model on our dataset\n",
    "# Note: In a real-world scenario, you'd use a much larger corpus\n",
    "# or pre-trained embeddings for better results\n",
    "w2v_model = Word2Vec(\n",
    "   X_train,\n",
    "   vector_size=100,  # Dimension of the embedding vectors\n",
    "   window=5,  # Context window size\n",
    "   min_count=1,  # Ignore words with fewer occurrences\n",
    "   workers=4,  # Number of processors to use\n",
    "   sg=1  # Skip-gram model (1) instead of CBOW (0)\n",
    ")\n",
    "\n",
    "\n",
    "# Function to create document vectors by averaging word vectors\n",
    "def document_to_vector(tokens, model, vector_size=100):\n",
    "   \"\"\"Convert a document (list of tokens) to a vector using word embeddings.\"\"\"\n",
    "   # Initialize an empty vector\n",
    "   doc_vector = np.zeros(vector_size)\n",
    "  \n",
    "   # Count valid tokens\n",
    "   valid_token_count = 0\n",
    "  \n",
    "   # Sum up vectors for each token\n",
    "   for token in tokens:\n",
    "       if token in model.wv:\n",
    "           doc_vector += model.wv[token]\n",
    "           valid_token_count += 1\n",
    "  \n",
    "   # Average the vectors\n",
    "   if valid_token_count > 0:\n",
    "       doc_vector /= valid_token_count\n",
    "      \n",
    "   return doc_vector\n",
    "\n",
    "\n",
    "def document_to_vector_pretrained(tokens, model, vector_size=300):\n",
    "   \"\"\"Convert a document (list of tokens) to a vector using word embeddings.\"\"\"\n",
    "   # Initialize an empty vector\n",
    "   doc_vector = np.zeros(vector_size)\n",
    "  \n",
    "   # Count valid tokens\n",
    "   valid_token_count = 0\n",
    "  \n",
    "   # Sum up vectors for each token\n",
    "   for token in tokens:\n",
    "       if token in model:\n",
    "           doc_vector += model[token]\n",
    "           valid_token_count += 1\n",
    "  \n",
    "   # Average the vectors\n",
    "   if valid_token_count > 0:\n",
    "       doc_vector /= valid_token_count\n",
    "      \n",
    "   return doc_vector\n",
    "\n",
    "\n",
    "# Create document vectors for training and test sets\n",
    "X_train_w2v = np.array([document_to_vector(tokens, w2v_model) for tokens in X_train_tokens])\n",
    "X_test_w2v = np.array([document_to_vector(tokens, w2v_model) for tokens in X_test_tokens])\n",
    "\n",
    "\n",
    "print(f\"Word2Vec document vectors shape: {X_train_w2v.shape}\")\n",
    "\n",
    "\n",
    "# Alternatively, download and use pre-trained embeddings\n",
    "# This takes more time but might give better results\n",
    "try:\n",
    "   # Attempt to download pre-trained embeddings (if internet is available)\n",
    "   pretrained_model = api.load('word2vec-google-news-300')\n",
    "   print(\"Pre-trained model loaded successfully.\")\n",
    "  \n",
    "   # Create vectors using pre-trained embeddings\n",
    "   X_train_pretrained = np.array([document_to_vector_pretrained(tokens, pretrained_model, 300)\n",
    "                                   for tokens in X_train_tokens])\n",
    "   X_test_pretrained = np.array([document_to_vector_pretrained(tokens, pretrained_model, 300)\n",
    "                                  for tokens in X_test_tokens])\n",
    "  \n",
    "   print(f\"Pre-trained document vectors shape: {X_train_pretrained.shape}\")\n",
    "   pretrained_available = True\n",
    "except Exception as e:\n",
    "   print(f\"Pre-trained embeddings could not be loaded: {e}\")\n",
    "   pretrained_available = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec064f0",
   "metadata": {},
   "source": [
    "##### Step 5: Model Selection and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541d3a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with Bag of Words features\n",
    "nb_bow = MultinomialNB(alpha=1.0)\n",
    "nb_bow.fit(X_train_counts, y_train)\n",
    "\n",
    "\n",
    "# Train with TF-IDF features\n",
    "nb_tfidf = MultinomialNB(alpha=1.0)\n",
    "nb_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "\n",
    "# For Word2Vec features, we need to convert the data to a non-negative representation\n",
    "# because Multinomial NB requires non-negative feature values\n",
    "# One simple approach is min-max scaling\n",
    "def min_max_scale(X):\n",
    "   \"\"\"Scale features to [0, 1] range.\"\"\"\n",
    "   X_min = X.min(axis=0)\n",
    "   X_max = X.max(axis=0)\n",
    "   return (X - X_min) / (X_max - X_min + 1e-10)  # Adding a small epsilon to avoid division by zero\n",
    "\n",
    "\n",
    "X_train_w2v_scaled = min_max_scale(X_train_w2v)\n",
    "X_test_w2v_scaled = min_max_scale(X_test_w2v)\n",
    "\n",
    "\n",
    "# Train with Word2Vec features\n",
    "nb_w2v = MultinomialNB(alpha=1.0)\n",
    "nb_w2v.fit(X_train_w2v_scaled, y_train)\n",
    "\n",
    "\n",
    "# Also train with pre-trained embeddings if available\n",
    "if pretrained_available:\n",
    "   X_train_pretrained_scaled = min_max_scale(X_train_pretrained)\n",
    "   X_test_pretrained_scaled = min_max_scale(X_test_pretrained)\n",
    "  \n",
    "   nb_pretrained = MultinomialNB(alpha=1.0)\n",
    "   nb_pretrained.fit(X_train_pretrained_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d16426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display top features by class\n",
    "def display_top_features(classifier, vectorizer, class_labels, n=10):\n",
    "   \"\"\"Display the top n words for each class based on their likelihood.\"\"\"\n",
    "   feature_names = vectorizer.get_feature_names_out()\n",
    "   for i, class_label in enumerate(class_labels):\n",
    "       top_indices = np.argsort(classifier.feature_log_prob_[i])[-n:][::-1]\n",
    "       top_features = [feature_names[j] for j in top_indices]\n",
    "       print(f\"\\nTop words for '{class_label}':\")\n",
    "       print(\", \".join(top_features))\n",
    "\n",
    "\n",
    "# Display top features for Bag of Words model\n",
    "print(\"\\nTop discriminative features per department (Bag of Words model):\")\n",
    "display_top_features(nb_bow, count_vectorizer, nb_bow.classes_)\n",
    "\n",
    "\n",
    "# Display top features for BoW model, focusing on bigrams\n",
    "print(\"\\nTop discriminative bigrams per department (Bag of Words model):\")\n",
    "def display_top_ngrams(classifier, vectorizer, class_labels, n=5):\n",
    "   \"\"\"Display the top n bigrams for each class based on their likelihood.\"\"\"\n",
    "   feature_names = vectorizer.get_feature_names_out()\n",
    "   # Get indices of all bigram features\n",
    "   bigram_indices = [i for i, feat in enumerate(feature_names) if ' ' in feat]\n",
    "  \n",
    "   for i, class_label in enumerate(class_labels):\n",
    "       # Filter to only consider bigram features\n",
    "       bigram_log_probs = [(j, classifier.feature_log_prob_[i][j]) for j in bigram_indices]\n",
    "       # Sort by probability (highest first)\n",
    "       top_bigram_indices = sorted(bigram_log_probs, key=lambda x: x[1], reverse=True)[:n]\n",
    "       top_bigrams = [feature_names[j] for j, _ in top_bigram_indices]\n",
    "       print(f\"\\nTop bigrams for '{class_label}':\")\n",
    "       print(\", \".join(top_bigrams))\n",
    "\n",
    "\n",
    "display_top_bigrams(nb_bow, count_vectorizer, nb_bow.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fda1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Function to visualize word embeddings in 2D\n",
    "def visualize_embeddings(model, words=None, n=50):\n",
    "   \"\"\"Visualize word embeddings in a 2D space.\"\"\"\n",
    "   # If no specific words provided, use most frequent words\n",
    "   if words is None:\n",
    "       # Get word counts from the corpus\n",
    "       from collections import Counter\n",
    "       all_words = [word for doc in df['tokenized_text'] for word in doc]\n",
    "       word_counts = Counter(all_words)\n",
    "       words = [word for word, count in word_counts.most_common(n)]\n",
    "  \n",
    "   # Filter for words in the model's vocabulary\n",
    "   valid_words = [word for word in words if word in model]\n",
    "  \n",
    "   if len(valid_words) == 0:\n",
    "       print(\"No valid words found in model vocabulary!\")\n",
    "       return\n",
    "  \n",
    "   # Get word vectors\n",
    "   word_vectors = [model[word] for word in valid_words]\n",
    "  \n",
    "   # Reduce to 2 dimensions with PCA\n",
    "   pca = PCA(n_components=2)\n",
    "   reduced_vectors = pca.fit_transform(word_vectors)\n",
    "  \n",
    "   # Plot the words in 2D space\n",
    "   plt.figure(figsize=(12, 10))\n",
    "   plt.scatter(reduced_vectors[:, 0], reduced_vectors[:, 1], s=10)\n",
    "  \n",
    "   # Add word labels\n",
    "   for i, word in enumerate(valid_words):\n",
    "       plt.annotate(word, (reduced_vectors[i, 0], reduced_vectors[i, 1]),\n",
    "                   fontsize=9, alpha=0.7)\n",
    "  \n",
    "   plt.title('Word Embeddings Visualization')\n",
    "   plt.tight_layout()\n",
    "   plt.show()\n",
    "\n",
    "\n",
    "# Visualize embeddings for department-specific words\n",
    "department_words = {\n",
    "   'technical': ['login', 'password', 'error', 'crash', 'app', 'loading', 'website'],\n",
    "   'account': ['update', 'change', 'information', 'address', 'settings', 'preferences'],\n",
    "   'billing': ['payment', 'charge', 'refund', 'credit', 'invoice', 'subscription']\n",
    "}\n",
    "\n",
    "\n",
    "# Flatten the word list\n",
    "all_dept_words = [word for words in department_words.values() for word in words]\n",
    "\n",
    "\n",
    "# Visualize the embeddings\n",
    "print(\"Visualizing word embeddings for department-specific terms:\")\n",
    "visualize_embeddings(pretrained_model, all_dept_words)\n",
    "\n",
    "\n",
    "# Also visualize some general support terms to see relationships\n",
    "general_terms = ['help', 'issue', 'problem', 'support', 'request', 'question',\n",
    "               'account', 'order', 'customer', 'service']\n",
    "visualize_embeddings(pretrained_model, general_terms + all_dept_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adc6f60",
   "metadata": {},
   "source": [
    "##### Step 5: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1b5c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform test data using the fitted vectorizers\n",
    "X_test_counts = count_vectorizer.transform(X_test)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "# Word2Vec test data was already prepared above\n",
    "\n",
    "\n",
    "# Predict with Bag of Words model\n",
    "y_pred_bow = nb_bow.predict(X_test_counts)\n",
    "\n",
    "\n",
    "# Predict with TF-IDF model\n",
    "y_pred_tfidf = nb_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "\n",
    "# Predict with Word2Vec model\n",
    "y_pred_w2v = nb_w2v.predict(X_test_w2v_scaled)\n",
    "\n",
    "\n",
    "# Predict with pre-trained embeddings if available\n",
    "y_pred_pretrained = nb_pretrained.predict(X_test_pretrained_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cd0421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate and display model performance\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "   \"\"\"Evaluate a model and print performance metrics.\"\"\"\n",
    "   accuracy = accuracy_score(y_true, y_pred)\n",
    "   print(f\"\\n{model_name} Model Performance:\")\n",
    "   print(f\"Accuracy: {accuracy:.4f}\")\n",
    "   print(\"\\nClassification Report:\")\n",
    "   print(classification_report(y_true, y_pred))\n",
    "  \n",
    "   # Create a confusion matrix\n",
    "   cm = confusion_matrix(y_true, y_pred)\n",
    "  \n",
    "   # Plot confusion matrix as a heatmap\n",
    "   plt.figure(figsize=(8, 6))\n",
    "   sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "              xticklabels=sorted(set(y_true)),\n",
    "              yticklabels=sorted(set(y_true)))\n",
    "   plt.title(f'Confusion Matrix - {model_name}')\n",
    "   plt.xlabel('Predicted')\n",
    "   plt.ylabel('True')\n",
    "   plt.tight_layout()\n",
    "   plt.show()\n",
    "  \n",
    "   return accuracy\n",
    "\n",
    "\n",
    "# Evaluate all models\n",
    "bow_accuracy = evaluate_model(y_test, y_pred_bow, \"Bag of Words\")\n",
    "tfidf_accuracy = evaluate_model(y_test, y_pred_tfidf, \"TF-IDF\")\n",
    "w2v_accuracy = evaluate_model(y_test, y_pred_w2v, \"Word2Vec\")\n",
    "\n",
    "\n",
    "if pretrained_available:\n",
    "   pretrained_accuracy = evaluate_model(y_test, y_pred_pretrained, \"Pre-trained Embeddings\")\n",
    "\n",
    "\n",
    "# Compare models\n",
    "models = [\"Bag of Words\", \"TF-IDF\", \"Word2Vec\"]\n",
    "accuracies = [bow_accuracy, tfidf_accuracy, w2v_accuracy]\n",
    "\n",
    "\n",
    "if pretrained_available:\n",
    "   models.append(\"Pre-trained Embeddings\")\n",
    "   accuracies.append(pretrained_accuracy)\n",
    "\n",
    "\n",
    "# Plot model comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=models, y=accuracies)\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "for i, acc in enumerate(accuracies):\n",
    "   plt.text(i, acc + 0.01, f'{acc:.4f}', ha='center')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad765a9",
   "metadata": {},
   "source": [
    "In this evaluation phase, we assess how well each of our models performs on the test data. We use several key metrics:\n",
    "\n",
    "1. Accuracy: The proportion of tickets correctly classified\n",
    "2. Precision: The proportion of positive identifications that were actually correct\n",
    "3. Recall: The proportion of actual positives that were identified correctly\n",
    "4, F1-score: The harmonic mean of precision and recall\n",
    "\n",
    "The confusion matrix provides a detailed view of where our model succeeds and where it makes mistakes. By comparing the performance of models built with different feature representations, we can understand the trade-offs between these approaches. \n",
    "\n",
    "Traditional Bag-of-Words and TF-IDF models often perform surprisingly well for text classification, while word embeddings can capture more semantic nuance but might require more data to be effective. \n",
    "\n",
    "Not surprisingly given the size of the dataset the pre-trained word embeddings model is performing the best in terms of classification accuracy. There is however extensive hyperparameter tuning that can be done for sklearn vectorizers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459d6434",
   "metadata": {},
   "source": [
    "##### Step 6: Model Refinement and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9f5b2d",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Create a pipeline for the TF-IDF approach\n",
    "tfidf_pipeline = Pipeline([\n",
    "   ('vectorizer', TfidfVectorizer(preprocessor=advanced_preprocess, lowercase=False)),\n",
    "   ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "\n",
    "# Define parameter grid to search\n",
    "param_grid = {\n",
    "   'vectorizer__min_df': [1, 2, 3],\n",
    "   'vectorizer__max_df': [0.9, 0.95, 1.0],\n",
    "   'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "   'classifier__alpha': [0.1, 0.5, 1.0, 2.0]\n",
    "}\n",
    "\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "   tfidf_pipeline,\n",
    "   param_grid,\n",
    "   cv=5,             # 5-fold cross-validation\n",
    "   scoring='accuracy',\n",
    "   verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# Fit the grid search to the data\n",
    "print(\"Performing grid search to optimize model hyperparameters...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Print best parameters and score\n",
    "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "\n",
    "# Evaluate the optimized model on test data\n",
    "optimized_pipeline = grid_search.best_estimator_\n",
    "y_pred_optimized = optimized_pipeline.predict(X_test)\n",
    "optimized_accuracy = evaluate_model(y_test, y_pred_optimized, \"Optimized TF-IDF\")\n",
    "\n",
    "\n",
    "# Add to our model comparison\n",
    "models.append(\"Optimized TF-IDF\")\n",
    "accuracies.append(optimized_accuracy)\n",
    "\n",
    "\n",
    "# Updated plot for model comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=models, y=accuracies)\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "for i, acc in enumerate(accuracies):\n",
    "   plt.text(i, acc + 0.01, f'{acc:.4f}', ha='center')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
